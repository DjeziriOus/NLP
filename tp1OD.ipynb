{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text author\n",
      "0  id26305  This process, however, afforded me no means of...    EAP\n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
      "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
      "4  id12958  Finding nothing else, not even gold, the Super...    HPL\n",
      "5  id22965  A youth passed in solitude, my best years spen...    MWS\n",
      "6  id09674  The astronomer, perhaps, at this point, took r...    EAP\n",
      "7  id13515        The surcingle hung in ribands from my body.    EAP\n",
      "8  id19322  I knew that you could not say to yourself 'ste...    EAP\n",
      "9  id00912  I confess that neither the structure of langua...    MWS\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL du jeu de données\n",
    "url = \"https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv\"\n",
    "\n",
    "# Importer le fichier CSV dans un DataFrame\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Afficher les 10 premiers échantillons\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def reduire_repetitions(texte):\n",
    "    \"\"\"\n",
    "    Cette fonction réduit toute séquence de caractères répétés à 2 occurrences.\n",
    "    Par exemple : \"cooooool\" devient \"cool\".\n",
    "    \"\"\"\n",
    "    # La fonction lambda remplace la séquence capturée par 2 occurrences du caractère\n",
    "    return re.sub(r'(.)\\1+', lambda m: m.group(1) * 2, texte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cool\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Exemple d'utilisation\n",
    "texte_initial = \"cooooool\"\n",
    "texte_nettoye = reduire_repetitions(texte_initial)\n",
    "print(texte_nettoye)  # Affiche \"cool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant normalisation : $tupide, c00l, l@zy, Αlpha, Вeta\n",
      "Après normalisation : stupide, cool, lazy, Alpha, Beta\n"
     ]
    }
   ],
   "source": [
    "# Dictionnaire de correspondances pour quelques homoglyphes\n",
    "homoglyphes = {\n",
    "    # Symboles spéciaux\n",
    "    '$': 's',\n",
    "    '@': 'a',\n",
    "    '0': 'o',\n",
    "    '1': 'i',  # selon le contexte, vous pouvez choisir 'i'\n",
    "    '!': 'i',\n",
    "    '¢': 'c',\n",
    "    '£': 'l',\n",
    "\n",
    "    # Lettres grecques\n",
    "    'Α': 'A',  # Alpha\n",
    "    'Β': 'B',  # Beta\n",
    "    'Ε': 'E',  # Epsilon\n",
    "    'Ζ': 'Z',  # Zeta\n",
    "    'Η': 'H',  # Eta\n",
    "    'Ι': 'I',  # Iota\n",
    "    'Κ': 'K',  # Kappa\n",
    "    'Μ': 'M',  # Mu\n",
    "    'Ν': 'N',  # Nu\n",
    "    'Ο': 'O',  # Omicron\n",
    "    'Ρ': 'P',  # Rho\n",
    "    'Τ': 'T',  # Tau\n",
    "    'Χ': 'X',  # Chi\n",
    "\n",
    "    # Lettres cyrilliques\n",
    "    'А': 'A',\n",
    "    'В': 'B',\n",
    "    'С': 'C',\n",
    "    'Е': 'E',\n",
    "    'Н': 'H',\n",
    "    'К': 'K',\n",
    "    'М': 'M',\n",
    "    'О': 'O',\n",
    "    'Р': 'P',\n",
    "    'Т': 'T',\n",
    "    'Х': 'X'\n",
    "}\n",
    "\n",
    "def normaliser_texte(texte):\n",
    "    \"\"\"\n",
    "    Remplace dans le texte les homoglyphes définis dans le dictionnaire par leur équivalent.\n",
    "    \"\"\"\n",
    "    for glyph, remplacement in homoglyphes.items():\n",
    "        texte = texte.replace(glyph, remplacement)\n",
    "    return texte\n",
    "\n",
    "# Exemple d'utilisation\n",
    "texte_exemple = \"$tupide, c00l, l@zy, Αlpha, Вeta\"\n",
    "texte_normalise = normaliser_texte(texte_exemple)\n",
    "\n",
    "print(\"Avant normalisation :\", texte_exemple)\n",
    "print(\"Après normalisation :\", texte_normalise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Assurez-vous d'avoir téléchargé la liste des stopwords pour le français\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Applique plusieurs étapes de prétraitement :\n",
    "      1. Remplacement des URL, adresses e-mail et suppression des balises HTML.\n",
    "      2. Conversion du texte en minuscules.\n",
    "      3. Suppression de la ponctuation.\n",
    "      4. Suppression des mots vides.\n",
    "    \"\"\"\n",
    "    # 1. Remplacer les URL par le token <URL>\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', 'url', text)\n",
    "    \n",
    "    # Remplacer les adresses e-mail par le token <EMAIL>\n",
    "    text = re.sub(r'\\S+@\\S+', 'email', text)\n",
    "    \n",
    "    # Supprimer les balises HTML (on peut aussi les remplacer par un token si besoin)\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # 2. Mettre tous les caractères en minuscule\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 3. Supprimer la ponctuation\n",
    "    # On utilise str.translate pour enlever tous les caractères de la chaîne string.punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 4. Supprimer les mots vides (stopwords) en français\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    # On tokenize simplement le texte par découpage sur les espaces\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Rejoindre les tokens pour obtenir le texte prétraité\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "# Exemple d'utilisation\n",
    "texte_exemple = (\"Bonjour, visitez notre site http://exemple.com pour plus d'infos. \"\n",
    "                 \"Contactez-nous à info@example.com. Voici un <b>texte en HTML</b> !\")\n",
    "texte_pretraite = preprocess_text(texte_exemple)\n",
    "\n",
    "print(\"Texte original :\")\n",
    "print(texte_exemple)\n",
    "print(\"\\nTexte prétraité :\")\n",
    "print(texte_pretraite)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Détecter les langues présentes dans le dataset et les traduire vers la langue la plus fréquente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Détection de la langue: 100%|██████████| 19579/19579 [00:39<00:00, 490.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution des langues détectées :\n",
      "lang\n",
      "en    19429\n",
      "fr       31\n",
      "nl       22\n",
      "af       13\n",
      "cy       11\n",
      "da       11\n",
      "it       11\n",
      "no        8\n",
      "de        8\n",
      "so        7\n",
      "ca        5\n",
      "pt        5\n",
      "tl        4\n",
      "sv        3\n",
      "es        3\n",
      "et        3\n",
      "tr        3\n",
      "id        2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "La langue la plus fréquente dans le dataset est : en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traduction des textes: 100%|██████████| 19579/19579 [01:04<00:00, 301.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exemple de lignes traitées :\n",
      "                                                text lang  \\\n",
      "0  This process, however, afforded me no means of...   en   \n",
      "1  It never once occurred to me that the fumbling...   en   \n",
      "2  In his left hand was a gold snuff box, from wh...   en   \n",
      "3  How lovely is spring As we looked from Windsor...   en   \n",
      "4  Finding nothing else, not even gold, the Super...   en   \n",
      "\n",
      "                                     translated_text  \n",
      "0  This process, however, afforded me no means of...  \n",
      "1  It never once occurred to me that the fumbling...  \n",
      "2  In his left hand was a gold snuff box, from wh...  \n",
      "3  How lovely is spring As we looked from Windsor...  \n",
      "4  Finding nothing else, not even gold, the Super...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect, DetectorFactory\n",
    "from googletrans import Translator\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pour rendre la détection de langue reproductible\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "\n",
    "# Supposons que la colonne contenant le texte s'appelle \"text\".\n",
    "# Si le nom de la colonne est différent, modifiez-le en conséquence.\n",
    "# Si besoin, inspectez les colonnes du DataFrame avec : print(df.columns)\n",
    "if 'text' not in df.columns:\n",
    "    raise ValueError(\"La colonne 'text' n'existe pas dans le dataset.\")\n",
    "\n",
    "# Fonction de détection de la langue avec gestion d'exception\n",
    "def detect_lang(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except Exception as e:\n",
    "        # En cas d'erreur (texte trop court, vide, etc.), on retourne None\n",
    "        return None\n",
    "\n",
    "# Détecter la langue pour chaque texte et stocker le résultat dans une nouvelle colonne\n",
    "tqdm.pandas(desc=\"Détection de la langue\")\n",
    "df['lang'] = df['text'].progress_apply(lambda x: detect_lang(str(x)))\n",
    "\n",
    "# Afficher la distribution des langues détectées\n",
    "lang_counts = df['lang'].value_counts(dropna=True)\n",
    "print(\"Distribution des langues détectées :\")\n",
    "print(lang_counts)\n",
    "\n",
    "# Identifier la langue la plus fréquente (en ignorant les éventuels None)\n",
    "most_common_lang = lang_counts.idxmax()\n",
    "print(\"\\nLa langue la plus fréquente dans le dataset est :\", most_common_lang)\n",
    "\n",
    "# Initialiser le traducteur\n",
    "translator = Translator()\n",
    "\n",
    "# Fonction pour traduire un texte vers la langue cible\n",
    "def translate_text(text, target_lang):\n",
    "    try:\n",
    "        # Si le texte est vide ou non de type string, on le retourne tel quel\n",
    "        if not text or not isinstance(text, str):\n",
    "            return text\n",
    "        # Traduction du texte\n",
    "        result = translator.translate(text, dest=target_lang)\n",
    "        return result.text\n",
    "    except Exception as e:\n",
    "        # En cas d'erreur lors de la traduction, afficher un message et renvoyer le texte original\n",
    "        print(\"Erreur lors de la traduction :\", e)\n",
    "        return text\n",
    "\n",
    "# Fonction de traitement d'une ligne : si la langue détectée n'est pas la langue cible,\n",
    "# on traduit le texte, sinon on le laisse inchangé.\n",
    "def process_row(row):\n",
    "    if row['lang'] != most_common_lang and row['lang'] is not None:\n",
    "        return translate_text(row['text'], most_common_lang)\n",
    "    else:\n",
    "        return row['text']\n",
    "\n",
    "# Traduire les textes qui ne sont pas dans la langue la plus fréquente\n",
    "tqdm.pandas(desc=\"Traduction des textes\")\n",
    "df['translated_text'] = df.progress_apply(process_row, axis=1)\n",
    "\n",
    "# Afficher les premières lignes avec le texte original, la langue détectée et le texte traduit\n",
    "print(\"\\nExemple de lignes traitées :\")\n",
    "print(df[['text', 'lang', 'translated_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exemple de lignes traduites :\n",
      "                                                  text  \\\n",
      "189  But these absurdities I must not pause to detail.   \n",
      "229                        Leave me; I am inexorable.\"   \n",
      "253  Xh, pxh, pxh, Jxhn, dxn't dx sx Yxu've gxt tx ...   \n",
      "294       There was Ferdinand Fitz Fossillus Feltspar.   \n",
      "310  There had been nothing like order or arrangement.   \n",
      "\n",
      "                                       translated_text  \n",
      "189  But these absurdities I must not pause to detail.  \n",
      "229                        Leave me; I am inexorable.\"  \n",
      "253  Xh, pxh, pxh, Jxhn, dxhn, dxn't dx Yexu gx x g...  \n",
      "294       There was Ferdinand Fitz Fossillus Feltspar.  \n",
      "310  There had been nothing like order or arrangement.  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExemple de lignes traduites :\")\n",
    "print(df.loc[df['lang'] != most_common_lang, ['text', 'translated_text']].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
