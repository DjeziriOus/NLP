{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nettoyage d’un texte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv\"\n",
    "# Load the dataset\n",
    "file = 'spooky.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the first 10 samples\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "repl = r'\\1\\2\\3'\n",
    "\n",
    "def replace(word):\n",
    "    if wordnet.synsets(word):\n",
    "        return word\n",
    "    repl_word = repeat_regexp.sub(repl, word)\n",
    "\n",
    "    if repl_word != word:\n",
    "        return replace(repl_word)\n",
    "    else:\n",
    "        return repl_word\n",
    "    \n",
    "def handle_repetitive_chars(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_words.append(replace(word))\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "df['text'] = df['text'].apply(handle_repetitive_chars)\n",
    "\n",
    "print(handle_repetitive_chars(\"cooooooool\"))\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homoglyphs_map = {\n",
    "    '$': 's', '0': 'o', '1': 'l', '3': 'e', '@': 'a', '5': 's', '7': 't', \n",
    "    '8': 'b', '|': 'i', '¢': 'c', '¡': 'i', 'µ': 'u', 'ß': 'b', '€': 'e',\n",
    "    '!':'i','£':'l',\n",
    "        \n",
    "    'Α': 'A', \n",
    "    'Β': 'B', \n",
    "    'Ε': 'E', \n",
    "    'Ζ': 'Z', \n",
    "    'Η': 'H', \n",
    "    'Ι': 'I', \n",
    "    'Κ': 'K', \n",
    "    'Μ': 'M', \n",
    "    'Ν': 'N', \n",
    "    'Ο': 'O', \n",
    "    'Ρ': 'P', \n",
    "    'Τ': 'T', \n",
    "    'Χ': 'X', \n",
    "\n",
    "    'А': 'A',\n",
    "    'В': 'B',\n",
    "    'С': 'C',\n",
    "    'Е': 'E',\n",
    "    'Н': 'H',\n",
    "    'К': 'K',\n",
    "    'М': 'M',\n",
    "    'О': 'O',\n",
    "    'Р': 'P',\n",
    "    'Т': 'T',\n",
    "    'Х': 'X'\n",
    "}\n",
    "\n",
    "def replace_homoglyphs(text):\n",
    "    return ''.join(homoglyphs_map.get(char, char) for char in text)\n",
    "\n",
    "\n",
    "print(replace_homoglyphs(\"$tupid\"))\n",
    "\n",
    "df['text'] = df['text'].apply(replace_homoglyphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_special_entries(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+', ' <URL> ', text)  \n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', ' <EMAIL> ', text)  \n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  \n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_special_entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "df['text'] = df['text'].apply(remove_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df['text'] = df['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from deep_translator import GoogleTranslator\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Example DataFrame (replace with your actual data)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Detect language for each text entry\n",
    "df['language'] = df['text'].apply(detect_language)\n",
    "\n",
    "# Determine the most frequent language in your dataset\n",
    "most_frequent_lang = Counter(df['language']).most_common(1)[0][0]\n",
    "\n",
    "# Cache translators to avoid creating a new instance each time\n",
    "translators = {}\n",
    "\n",
    "def translate_text(text, src_lang):\n",
    "    # Only translate if the source language is not the majority and not unknown\n",
    "    if src_lang != most_frequent_lang and src_lang != \"unknown\":\n",
    "        # Use a cached translator if available; otherwise, create and cache one\n",
    "        if src_lang not in translators:\n",
    "            translators[src_lang] = GoogleTranslator(source=src_lang, target=most_frequent_lang)\n",
    "        return translators[src_lang].translate(text)\n",
    "    return text\n",
    "\n",
    "# Apply translation row by row\n",
    "df['text'] = df.apply(lambda row: translate_text(row['text'], row['language']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda text: re.sub(r'[^a-zA-Z0-9\\s]', '', text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_words(text):\n",
    "    words = text.split()\n",
    "    return \" \".join(sorted(set(words), key=words.index))  # Keeps first occurrence of each word\n",
    "\n",
    "df['text'] = df['text'].apply(remove_repeated_words)\n",
    "df['text'] = df['text'].str.strip()  # Remove leading/trailing spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cleaned_spooky.csv\", index=False)\n",
    "print(\"Preprocessing complete. Data saved as cleaned_spooky.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import spacy\n",
    "\n",
    "# Download the necessary NLTK tokenizer data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load SpaCy English model (only needed if you plan to use SpaCy later)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read the CSV file\n",
    "file = 'cleaned_spooky.csv'\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "print(\"First 10 rows before cleaning:\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Drop rows where the 'text' column is NaN\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Option A: Using NLTK's word_tokenize for tokenization\n",
    "df['tokens'] = df['text'].apply(word_tokenize)\n",
    "\n",
    "# Option B (Alternative): Using SpaCy's tokenizer with nlp.pipe (better for large datasets)\n",
    "# Uncomment the following block and comment out Option A if you prefer SpaCy\n",
    "# tokens = []\n",
    "# for doc in nlp.pipe(df['text'], batch_size=50, n_process=-1):\n",
    "#     tokens.append([token.text for token in doc])\n",
    "# df['tokens'] = tokens\n",
    "\n",
    "print(\"First 10 rows after tokenization:\")\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def rule_based_tokenization(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "df['rule_based_tokens'] = df['text'].apply(rule_based_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def subword_tokenization(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "df['subword_tokens'] = df['text'].apply(subword_tokenization)\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Reconnaissance d'entité nommée "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "df['entities'] = df['text'].apply(extract_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "df['pos_tags'] = df['text'].apply(pos_tagging)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. Réduction des formes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def stem_text(text):\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "df['lemmatized_text'] = df['text'].apply(lemmatize_text)\n",
    "df['stemmed_text'] = df['text'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Analyse des fréquences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['contains_great'] = df['text'].apply(lambda x: 'great' in x.lower())\n",
    "great_counts = df.groupby('author')['contains_great'].sum()\n",
    "print(great_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pywaffle import Waffle\n",
    "\n",
    "data = great_counts.to_dict()\n",
    "fig = plt.figure(\n",
    "    FigureClass=Waffle,\n",
    "    rows=5,\n",
    "    columns=10,\n",
    "    values=data,\n",
    "    legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)},\n",
    "    title={'label': 'Occurrences of \"great\" by Author', 'loc': 'left'}\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['contains_impossible'] = df['text'].apply(lambda x: 'impossible' in x.lower())\n",
    "impossible_counts = df.groupby('author')['contains_impossible'].sum()\n",
    "print(impossible_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = impossible_counts.to_dict()\n",
    "\n",
    "if sum(data.values()) == 0:\n",
    "    print(\"No data available to plot the waffle chart.\")\n",
    "else:\n",
    "    fig = plt.figure(\n",
    "        FigureClass=Waffle,\n",
    "        rows=5,\n",
    "        columns=10,\n",
    "        values=data,\n",
    "        legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)},\n",
    "        title={'label': 'Occurrences of \"impossible\" by Author', 'loc': 'left'}\n",
    "    )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for author in df['author'].unique():\n",
    "    text = ' '.join(df[df['author'] == author]['text'])\n",
    "    wordcloud = WordCloud(width=800, height=400).generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Word Cloud for {author}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_words(text):\n",
    "    words = text.split()\n",
    "    positive_words = [word for word in words if sia.polarity_scores(word)['compound'] > 0]\n",
    "    negative_words = [word for word in words if sia.polarity_scores(word)['compound'] < 0]\n",
    "    return positive_words, negative_words\n",
    "\n",
    "df['positive_words'], df['negative_words'] = zip(*df['text'].apply(get_sentiment_words))\n",
    "\n",
    "# Get top 100 positive and negative words\n",
    "all_positive_words = [word for sublist in df['positive_words'] for word in sublist]\n",
    "all_negative_words = [word for sublist in df['negative_words'] for word in sublist]\n",
    "\n",
    "top_positive_words = pd.Series(all_positive_words).value_counts().head(100)\n",
    "top_negative_words = pd.Series(all_negative_words).value_counts().head(100)\n",
    "# Generate WordCloud for positive words\n",
    "\n",
    "\n",
    "# Generate WordCloud for top positive words\n",
    "positive_wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Greens').generate_from_frequencies(top_positive_words.to_dict())\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Top 100 Positive Words\")\n",
    "plt.show()\n",
    "\n",
    "# Generate WordCloud for top negative words\n",
    "negative_wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate_from_frequencies(top_negative_words.to_dict())\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Top 100 Negative Words\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
